{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72Q_qPRZqgmd",
        "outputId": "d211545c-c2c8-4ad3-a5f4-f258cbdf75e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl.metadata (943 bytes)\n",
            "Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pygame pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHxHLJiyrXwd",
        "outputId": "d4c69004-e83b-4cc6-9cec-3e25cf6062ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 2s (579 kB/s)\n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 126319 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Collecting box2d\n",
            "  Downloading Box2D-2.3.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (573 bytes)\n",
            "Collecting box2d-kengz\n",
            "  Downloading Box2D-kengz-2.3.3.tar.gz (425 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.4/425.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.14.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n",
            "Downloading Box2D-2.3.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-kengz\n",
            "  Building wheel for box2d-kengz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-kengz: filename=Box2D_kengz-2.3.3-cp311-cp311-linux_x86_64.whl size=2369377 sha256=279c58ba22e9e33cdd23552d8a37318fc4615d9ccc4a9977e33c626a4e136fc3\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/6a/a7/b49372347210a86f54c6603329c24c48dba26c98586ae80fbb\n",
            "Successfully built box2d-kengz\n",
            "Installing collected packages: box2d-kengz, box2d\n",
            "Successfully installed box2d-2.3.10 box2d-kengz-2.3.3\n"
          ]
        }
      ],
      "source": [
        "!apt install swig && pip install gymnasium box2d box2d-kengz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HZtoaO8loTWQ"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from IPython.display import display, HTML\n",
        "import base64\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIYGhHHMsv-Y",
        "outputId": "1896b5e0-1f43-48a3-ee39-68d746d8d18e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
            "Discrete(2)\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "print(env.observation_space)\n",
        "print(env.action_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cp2osgpMtA5x",
        "outputId": "03d5d634-f1b1-4201-ae59-927a72d97e0e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4,)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "env.observation_space.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3NaH4k4tb9f",
        "outputId": "2a356acd-641c-4226-ed61-b32a1044d4e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-4.8         4.8       ]\n",
            " [       -inf         inf]\n",
            " [-0.41887903  0.41887903]\n",
            " [       -inf         inf]]\n"
          ]
        }
      ],
      "source": [
        "obs_space_low_ar = env.observation_space.low\n",
        "obs_space_high_ar = env.observation_space.high\n",
        "arrays = []\n",
        "for i in range(len(obs_space_low_ar)):\n",
        "    arrays.append([obs_space_low_ar[i], obs_space_high_ar[i]])\n",
        "obs_bounds = np.array(arrays)\n",
        "print(obs_bounds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0b3BtvQ10Z7h"
      },
      "outputs": [],
      "source": [
        "practical_state_bounds = np.array([[-4.8, 4.8], [-10, 10], [-0.41887903, 0.41887903], [-10, 10]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "EJWV8rmRwRKT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class TileCoder:\n",
        "    def __init__(self, num_tilings, tiles_per_dim, state_bounds):\n",
        "        self.num_tilings = num_tilings\n",
        "        self.tiles_per_dim = tiles_per_dim\n",
        "        self.state_bounds = state_bounds  # shape: (D, 2), where D = state dimensions\n",
        "\n",
        "        self.state_dim = len(state_bounds)\n",
        "        self.tile_width = (self.state_bounds[:, 1] - self.state_bounds[:, 0]) / (tiles_per_dim - 1)\n",
        "        self.offsets = [np.linspace(0, self.tile_width[d], num_tilings) for d in range(self.state_dim)]\n",
        "\n",
        "        self.total_tiles = (tiles_per_dim ** self.state_dim) * num_tilings\n",
        "\n",
        "    def get_features(self, state):\n",
        "        \"\"\"\n",
        "        Return binary feature vector for given state only (not action)\n",
        "        \"\"\"\n",
        "        features = np.zeros(self.total_tiles)\n",
        "        for tiling in range(self.num_tilings):\n",
        "            coords = []\n",
        "            for i in range(self.state_dim):\n",
        "                offset = self.offsets[i][tiling]\n",
        "                coord = int((state[i] - self.state_bounds[i][0] + offset) / self.tile_width[i])\n",
        "                coords.append(coord)\n",
        "\n",
        "            flat_index = self._tile_index(tiling, coords)\n",
        "            features[flat_index] = 1\n",
        "\n",
        "        return features.reshape(-1)\n",
        "\n",
        "    def _tile_index(self, tiling, coords):\n",
        "        \"\"\"\n",
        "        Compute flattened index in the feature vector (no action)\n",
        "        \"\"\"\n",
        "        index = 0\n",
        "        for c in coords:\n",
        "            index = index * self.tiles_per_dim + c\n",
        "        index += tiling * (self.tiles_per_dim ** self.state_dim)\n",
        "        return index\n",
        "tile_coder = TileCoder(num_tilings=8, tiles_per_dim=8, state_bounds=practical_state_bounds)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations_with_replacement\n",
        "\n",
        "def polynomial_features(state, degree=3):\n",
        "    \"\"\"\n",
        "    Generate a polynomial basis feature vector from a 4D state.\n",
        "\n",
        "    Parameters:\n",
        "        state: array-like\n",
        "        degree: int, the maximum total degree of polynomial terms\n",
        "\n",
        "    Returns:\n",
        "        feature_vector: np.ndarray, shape (num_features,)\n",
        "    \"\"\"\n",
        "    state = np.asarray(state)\n",
        "    size = state.shape[0]\n",
        "\n",
        "    # Create index tuples for all monomials up to the given degree\n",
        "    feature_vector = [1.0]  # bias term\n",
        "    for d in range(1, degree + 1):\n",
        "        for idxs in combinations_with_replacement(range(size), d):\n",
        "            term = 1.0\n",
        "            for i in idxs:\n",
        "                term *= state[i]\n",
        "            feature_vector.append(term)\n",
        "\n",
        "    return np.array(feature_vector).reshape(-1)\n"
      ],
      "metadata": {
        "id": "ZJri0mEfef9J"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iqbNUfMuBlp"
      },
      "source": [
        "The action space is discrete with actions taking on values of 0 or 1 . The observation space is a continuous space in R^4. The second and fourth dimensions are unbounded but I will be clipping them.  This notebook is an implementation of the REINFORCE with Baseline algorithm. The baseline is a function approximatation of the state value using a linear function. The parameterized policy will be a Neural Network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "glm35cNtzUfe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "alpha_sv_estimates = .02\n",
        "alpha_policy_updates = .015\n",
        "gamma = .99 #discount factor on future rewards\n",
        "num_episodes = 50000\n",
        "dim = polynomial_features(env.observation_space.sample()).shape[0]\n",
        "wts = np.random.normal(size=dim)\n",
        "num_actions = env.action_space.n\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, action_dim)\n",
        "\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "\n",
        "        if torch.isnan(x).any():\n",
        "          print(\"NaNs detected in output of layer 1!\")\n",
        "\n",
        "        if torch.isinf(x).any():\n",
        "          print(\"Infs detected in output of layer 1!\")\n",
        "        logits = self.fc2(x)\n",
        "\n",
        "        if torch.isnan(logits).any():\n",
        "          print(\"NaNs detected in logits!\")\n",
        "\n",
        "        if torch.isinf(logits).any():\n",
        "          print(\"Infs detected in logits!\")\n",
        "\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        return probs\n",
        "\n",
        "def get_features(s):\n",
        "  clipped_state = np.clip(s, practical_state_bounds[:, 0], practical_state_bounds[:, 1])\n",
        "  return tile_coder.get_features(clipped_state)\n",
        "\n",
        "def gradient_descent_sv_weights(w, G, s):\n",
        "  features = polynomial_features(s)\n",
        "  w += alpha_sv_estimates * (G - np.dot(w, features)) * features\n",
        "  return w\n",
        "\n",
        "def select_action(policy_net, state):\n",
        "    features = polynomial_features(state)\n",
        "    if np.isnan(features).any():\n",
        "      print(\"NaNs in features!\")\n",
        "    input = torch.tensor(features, dtype=torch.float32).to(device)\n",
        "    probs = policy_net(input)\n",
        "    m = torch.distributions.Categorical(probs)\n",
        "    action = m.sample()\n",
        "    log_prob = m.log_prob(action)\n",
        "    return action.item(), log_prob\n",
        "\n",
        "def get_log_prob(s, a):\n",
        "  features = polynomial_features(s)\n",
        "  if np.isnan(features).any():\n",
        "    print(\"NaNs in features!\")\n",
        "  input = torch.tensor(features, dtype=torch.float32)\n",
        "  probs = policy_network(input)\n",
        "  m = torch.distributions.Categorical(probs)\n",
        "  log_prob = m.log_prob(torch.tensor(a, dtype=torch.long))\n",
        "  return log_prob\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yElMuUJjCEb2",
        "outputId": "c6be53a6-4fe1-4888-9541-d3c512332604"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17.0\n",
            "19.0\n",
            "27.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "9.0\n",
            "8.0\n",
            "8.0\n",
            "10.0\n",
            "9.0\n",
            "10.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "8.0\n",
            "9.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "11.0\n",
            "10.0\n",
            "11.0\n",
            "9.0\n",
            "9.0\n",
            "10.0\n",
            "9.0\n",
            "8.0\n",
            "8.0\n",
            "10.0\n",
            "9.0\n",
            "8.0\n",
            "10.0\n",
            "10.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n"
          ]
        }
      ],
      "source": [
        "policy_network = PolicyNetwork(dim, num_actions).to(device)\n",
        "optimizer = torch.optim.Adam(policy_network.parameters(), lr=alpha_policy_updates)\n",
        "\n",
        "for episode_idx in range(num_episodes):\n",
        "    rewards = []\n",
        "    actions = []\n",
        "    states = []\n",
        "    log_probs = []\n",
        "\n",
        "    obs, _ = env.reset()\n",
        "    undiscounted_reward_sum = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        states.append(obs)\n",
        "        action, log_prob = select_action(policy_network, obs)\n",
        "        obs, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "        undiscounted_reward_sum += reward\n",
        "        rewards.append(reward)\n",
        "        actions.append(action)\n",
        "        log_probs.append(log_prob)\n",
        "\n",
        "        done = terminated or truncated\n",
        "\n",
        "    if episode_idx % 1000 == 0:\n",
        "        print(undiscounted_reward_sum)\n",
        "\n",
        "    # Compute discounted returns\n",
        "    returns = []\n",
        "    G = 0\n",
        "    for r in reversed(rewards):\n",
        "        G = r + gamma * G\n",
        "        returns.insert(0, G)\n",
        "\n",
        "    # Compute advantages and update baseline weights\n",
        "    advantages = []\n",
        "    for i in range(len(states)):\n",
        "        cur_state = states[i]\n",
        "        cur_return = returns[i]\n",
        "        feat_np = polynomial_features(cur_state)\n",
        "        baseline = np.dot(wts, feat_np)\n",
        "        adv = cur_return - baseline\n",
        "        advantages.append(adv)\n",
        "\n",
        "        # Update linear baseline (on CPU)\n",
        "        wts = gradient_descent_sv_weights(wts, cur_return, cur_state)\n",
        "\n",
        "    # Convert to GPU tensors\n",
        "    log_probs = torch.stack(log_probs).to(device)\n",
        "    advantages = torch.tensor(advantages, dtype=torch.float32, device=device)\n",
        "\n",
        "    # Normalize advantages for stability (optional but helpful)\n",
        "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "    # Compute batched policy loss and backprop\n",
        "    loss = -(log_probs * advantages).mean()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paD9tE1ZETyb",
        "outputId": "65acf2e0-931e-4940-ab51-e970d10d066c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11.0, 12.0, 10.0, 12.0, 12.0, 12.0, 12.0, 9.0, 11.0, 11.0, 11.0, 13.0, 12.0, 11.0, 11.0, 12.0, 10.0, 9.0, 9.0, 11.0, 11.0, 9.0, 12.0, 11.0, 11.0, 12.0, 9.0, 10.0, 12.0, 10.0, 11.0, 13.0, 12.0, 9.0, 13.0, 11.0, 11.0, 11.0, 13.0, 13.0, 10.0, 9.0, 10.0, 13.0, 13.0, 11.0, 11.0, 11.0, 11.0, 12.0]\n"
          ]
        }
      ],
      "source": [
        "rewards = []\n",
        "for episode_idx in range(50):\n",
        "    obs, _ = env.reset()\n",
        "    reward_sum = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action, log_prob = select_action(policy_network, obs)\n",
        "        obs, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "        reward_sum += reward\n",
        "\n",
        "        done = terminated or truncated\n",
        "\n",
        "    rewards.append(reward_sum)\n",
        "\n",
        "print(rewards)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aB9SzO9LH34T",
        "outputId": "349f441a-d77d-474e-a736-feb01e04a870"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[13.0, 11.0, 12.0, 13.0, 9.0, 10.0, 10.0, 11.0, 9.0, 11.0, 11.0, 9.0, 12.0, 11.0, 10.0, 9.0, 9.0, 12.0, 11.0, 12.0, 9.0, 11.0, 10.0, 11.0, 12.0]\n"
          ]
        }
      ],
      "source": [
        "def select_greedy_action(policy_net, state):\n",
        "    features = get_features(state)\n",
        "    input = torch.tensor(features, dtype=torch.float32).to(device)\n",
        "    probs = policy_net(input)\n",
        "    m = torch.distributions.Categorical(probs)\n",
        "    return m.probs.argmax().item()\n",
        "\n",
        "rewards = []\n",
        "for episode_idx in range(25):\n",
        "    obs, _ = env.reset()\n",
        "    reward_sum = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action = select_greedy_action(policy_network, obs)\n",
        "        obs, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "        reward_sum += reward\n",
        "\n",
        "        done = terminated or truncated\n",
        "\n",
        "    rewards.append(reward_sum)\n",
        "\n",
        "print(rewards)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tErsqwJcLTNS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOIW3xtBh3YHHSE/oak9AKo"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}