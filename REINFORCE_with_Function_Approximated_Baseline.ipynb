{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMM+XVzM83a6fu9IvlCg45m"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pygame pyvirtualdisplay"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72Q_qPRZqgmd",
        "outputId": "421f0b36-75a5-418b-e7f7-02ddadf3b1a6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl.metadata (943 bytes)\n",
            "Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install swig && pip install gymnasium box2d box2d-kengz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHxHLJiyrXwd",
        "outputId": "78da3c7e-ea42-41f2-aab3-36d4cdf50ce9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 3s (442 kB/s)\n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 126319 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Collecting box2d\n",
            "  Downloading Box2D-2.3.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (573 bytes)\n",
            "Collecting box2d-kengz\n",
            "  Downloading Box2D-kengz-2.3.3.tar.gz (425 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.4/425.4 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.14.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n",
            "Downloading Box2D-2.3.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m117.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-kengz\n",
            "  Building wheel for box2d-kengz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-kengz: filename=Box2D_kengz-2.3.3-cp311-cp311-linux_x86_64.whl size=2369375 sha256=28f803125c461a012566119b0ed768a3f0dd296d727977ba1e0857cdcb056895\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/6a/a7/b49372347210a86f54c6603329c24c48dba26c98586ae80fbb\n",
            "Successfully built box2d-kengz\n",
            "Installing collected packages: box2d-kengz, box2d\n",
            "Successfully installed box2d-2.3.10 box2d-kengz-2.3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HZtoaO8loTWQ"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from IPython.display import display, HTML\n",
        "import base64\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "print(env.observation_space)\n",
        "print(env.action_space)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIYGhHHMsv-Y",
        "outputId": "12c0ae7e-9af9-40bf-9c05-92f2a8899fb4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
            "Discrete(2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.observation_space.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cp2osgpMtA5x",
        "outputId": "2633cb78-e190-4fba-8b92-31166c7d8f68"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4,)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "obs_space_low_ar = env.observation_space.low\n",
        "obs_space_high_ar = env.observation_space.high\n",
        "arrays = []\n",
        "for i in range(len(obs_space_low_ar)):\n",
        "    arrays.append([obs_space_low_ar[i], obs_space_high_ar[i]])\n",
        "obs_bounds = np.array(arrays)\n",
        "print(obs_bounds)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3NaH4k4tb9f",
        "outputId": "15e3435f-5a66-4793-eb97-38928d9257b0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-4.8         4.8       ]\n",
            " [       -inf         inf]\n",
            " [-0.41887903  0.41887903]\n",
            " [       -inf         inf]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "practical_state_bounds = np.array([[-4.8, 4.8], [-10, 10], [-0.41887903, 0.41887903], [-10, 10]])"
      ],
      "metadata": {
        "id": "0b3BtvQ10Z7h"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class TileCoder:\n",
        "    def __init__(self, num_tilings, tiles_per_dim, state_bounds):\n",
        "        self.num_tilings = num_tilings\n",
        "        self.tiles_per_dim = tiles_per_dim\n",
        "        self.state_bounds = state_bounds  # shape: (D, 2), where D = state dimensions\n",
        "\n",
        "        self.state_dim = len(state_bounds)\n",
        "        self.tile_width = (self.state_bounds[:, 1] - self.state_bounds[:, 0]) / (tiles_per_dim - 1)\n",
        "        self.offsets = [np.linspace(0, self.tile_width[d], num_tilings) for d in range(self.state_dim)]\n",
        "\n",
        "        self.total_tiles = (tiles_per_dim ** self.state_dim) * num_tilings\n",
        "\n",
        "    def get_features(self, state):\n",
        "        \"\"\"\n",
        "        Return binary feature vector for given state only (not action)\n",
        "        \"\"\"\n",
        "        features = np.zeros(self.total_tiles)\n",
        "        for tiling in range(self.num_tilings):\n",
        "            coords = []\n",
        "            for i in range(self.state_dim):\n",
        "                offset = self.offsets[i][tiling]\n",
        "                coord = int((state[i] - self.state_bounds[i][0] + offset) / self.tile_width[i])\n",
        "                coords.append(coord)\n",
        "\n",
        "            flat_index = self._tile_index(tiling, coords)\n",
        "            features[flat_index] = 1\n",
        "\n",
        "        return features.reshape(-1)\n",
        "\n",
        "    def _tile_index(self, tiling, coords):\n",
        "        \"\"\"\n",
        "        Compute flattened index in the feature vector (no action)\n",
        "        \"\"\"\n",
        "        index = 0\n",
        "        for c in coords:\n",
        "            index = index * self.tiles_per_dim + c\n",
        "        index += tiling * (self.tiles_per_dim ** self.state_dim)\n",
        "        return index\n",
        "tile_coder = TileCoder(num_tilings=8, tiles_per_dim=8, state_bounds=practical_state_bounds)"
      ],
      "metadata": {
        "id": "EJWV8rmRwRKT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The action space is discrete with actions taking on values of 0 or 1 . The observation space is a continuous space in R^4. The second and fourth dimensions are unbounded but I will be clipping them.  This notebook is an implementation of the REINFORCE with Baseline algorithm. The baseline is a function approximatation of the state value using a linear function. The parameterized policy will be a Neural Network."
      ],
      "metadata": {
        "id": "_iqbNUfMuBlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "alpha_sv_estimates = .02\n",
        "alpha_policy_updates = .015\n",
        "gamma = .99 #discount factor on future rewards\n",
        "num_episodes = 15000\n",
        "dim = tile_coder.total_tiles\n",
        "wts = np.random.normal(size=dim)\n",
        "num_actions = env.action_space.n\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, action_dim)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        logits = self.fc2(x)\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        return probs\n",
        "\n",
        "def get_features(s):\n",
        "  clipped_state = np.clip(s, practical_state_bounds[:, 0], practical_state_bounds[:, 1])\n",
        "  return tile_coder.get_features(clipped_state)\n",
        "\n",
        "def gradient_descent_sv_weights(w, G, s):\n",
        "  features = get_features(s)\n",
        "  w += alpha_sv_estimates * (G - np.dot(w, features)) * features\n",
        "  return w\n",
        "\n",
        "def select_action(policy_net, state):\n",
        "    features = get_features(state)\n",
        "    input = torch.tensor(features, dtype=torch.float32).to(device)\n",
        "    probs = policy_net(input)\n",
        "    m = torch.distributions.Categorical(probs)\n",
        "    action = m.sample()\n",
        "    log_prob = m.log_prob(action)\n",
        "    return action.item(), log_prob\n",
        "\n",
        "def get_log_prob(s, a):\n",
        "  features = get_features(s)\n",
        "  input = torch.tensor(features, dtype=torch.float32)\n",
        "  probs = policy_network(input)\n",
        "  m = torch.distributions.Categorical(probs)\n",
        "  log_prob = m.log_prob(torch.tensor(a, dtype=torch.long))\n",
        "  return log_prob\n",
        "\n",
        "policy_network = PolicyNetwork(dim, num_actions).to(device)\n",
        "optimizer = torch.optim.Adam(policy_network.parameters(), lr=alpha_policy_updates)\n"
      ],
      "metadata": {
        "id": "glm35cNtzUfe"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''for episode_idx in range(num_episodes):\n",
        "  rewards = []\n",
        "  actions = []\n",
        "  states = []\n",
        "  obs, _ = env.reset() #reset the env to get initial state\n",
        "  undiscounted_reward_sum = 0\n",
        "  done = False\n",
        "  while not done:\n",
        "      states.append(obs)\n",
        "      action, log_prob = select_action(policy_network, obs)\n",
        "      obs, reward, terminated, truncated, _ = env.step(action)\n",
        "      undiscounted_reward_sum += reward\n",
        "      rewards.append(reward)\n",
        "      actions.append(action)\n",
        "      done = terminated or truncated\n",
        "  if episode_idx % 100 == 0:\n",
        "    print(undiscounted_reward_sum)\n",
        "  for i in range(len(states)):\n",
        "    #loop thru each state\n",
        "    cur_state = states[i]\n",
        "    cur_reward = 0\n",
        "    for j in range(i, len(rewards)):\n",
        "      cur_reward += gamma**(j-i) * rewards[j]\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss = -1 * get_log_prob(cur_state, actions[i]) * (cur_reward - np.dot(wts, get_features(cur_state)))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    wts = gradient_descent_sv_weights(wts, cur_reward, cur_state)\n",
        "\n",
        "\n",
        "env.close()'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "hUlMf5-sqy0A",
        "outputId": "0ae7028d-8028-4f03-9cef-3c56dc4d521a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20.0\n",
            "9.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-18-3111992141.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mget_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcur_reward\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mwts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_descent_sv_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m                             )\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    242\u001b[0m             )\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    245\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    874\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    877\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    474\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for episode_idx in range(num_episodes):\n",
        "    rewards = []\n",
        "    actions = []\n",
        "    states = []\n",
        "    log_probs = []\n",
        "\n",
        "    obs, _ = env.reset()\n",
        "    undiscounted_reward_sum = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        states.append(obs)\n",
        "        action, log_prob = select_action(policy_network, obs)\n",
        "        obs, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "        undiscounted_reward_sum += reward\n",
        "        rewards.append(reward)\n",
        "        actions.append(action)\n",
        "        log_probs.append(log_prob)\n",
        "\n",
        "        done = terminated or truncated\n",
        "\n",
        "    if episode_idx % 100 == 0:\n",
        "        print(undiscounted_reward_sum)\n",
        "\n",
        "    # Compute discounted returns\n",
        "    returns = []\n",
        "    G = 0\n",
        "    for r in reversed(rewards):\n",
        "        G = r + gamma * G\n",
        "        returns.insert(0, G)\n",
        "\n",
        "    # Compute advantages and update baseline weights\n",
        "    advantages = []\n",
        "    for i in range(len(states)):\n",
        "        cur_state = states[i]\n",
        "        cur_return = returns[i]\n",
        "        feat_np = get_features(cur_state)\n",
        "        baseline = np.dot(wts, feat_np)\n",
        "        adv = cur_return - baseline\n",
        "        advantages.append(adv)\n",
        "\n",
        "        # Update linear baseline (on CPU)\n",
        "        wts = gradient_descent_sv_weights(wts, cur_return, cur_state)\n",
        "\n",
        "    # Convert to GPU tensors\n",
        "    log_probs = torch.stack(log_probs).to(device)\n",
        "    advantages = torch.tensor(advantages, dtype=torch.float32, device=device)\n",
        "\n",
        "    # Normalize advantages for stability (optional but helpful)\n",
        "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "    # Compute batched policy loss and backprop\n",
        "    loss = -(log_probs * advantages).mean()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yElMuUJjCEb2",
        "outputId": "b80b853d-ca63-4e36-c7c3-51868c55454f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18.0\n",
            "10.0\n",
            "10.0\n",
            "11.0\n",
            "10.0\n",
            "9.0\n",
            "10.0\n",
            "9.0\n",
            "8.0\n",
            "10.0\n",
            "10.0\n",
            "9.0\n",
            "10.0\n",
            "9.0\n",
            "8.0\n",
            "11.0\n",
            "9.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "9.0\n",
            "8.0\n",
            "8.0\n",
            "8.0\n",
            "11.0\n",
            "8.0\n",
            "10.0\n",
            "10.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "9.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "11.0\n",
            "9.0\n",
            "10.0\n",
            "8.0\n",
            "9.0\n",
            "9.0\n",
            "11.0\n",
            "9.0\n",
            "9.0\n",
            "10.0\n",
            "10.0\n",
            "8.0\n",
            "9.0\n",
            "10.0\n",
            "9.0\n",
            "9.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "8.0\n",
            "9.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "9.0\n",
            "8.0\n",
            "10.0\n",
            "10.0\n",
            "10.0\n",
            "9.0\n",
            "17.0\n",
            "18.0\n",
            "41.0\n",
            "24.0\n",
            "22.0\n",
            "28.0\n",
            "29.0\n",
            "26.0\n",
            "28.0\n",
            "28.0\n",
            "41.0\n",
            "53.0\n",
            "39.0\n",
            "22.0\n",
            "23.0\n",
            "54.0\n",
            "49.0\n",
            "34.0\n",
            "18.0\n",
            "25.0\n",
            "22.0\n",
            "17.0\n",
            "49.0\n",
            "51.0\n",
            "44.0\n",
            "61.0\n",
            "41.0\n",
            "39.0\n",
            "32.0\n",
            "94.0\n",
            "105.0\n",
            "56.0\n",
            "67.0\n",
            "52.0\n",
            "92.0\n",
            "96.0\n",
            "89.0\n",
            "104.0\n",
            "34.0\n",
            "100.0\n",
            "111.0\n",
            "101.0\n",
            "93.0\n",
            "96.0\n",
            "167.0\n",
            "171.0\n",
            "92.0\n",
            "110.0\n",
            "98.0\n",
            "96.0\n",
            "102.0\n",
            "98.0\n",
            "118.0\n",
            "102.0\n",
            "106.0\n",
            "104.0\n",
            "102.0\n",
            "88.0\n",
            "101.0\n",
            "110.0\n",
            "106.0\n",
            "110.0\n",
            "89.0\n",
            "88.0\n",
            "87.0\n",
            "98.0\n",
            "114.0\n",
            "104.0\n",
            "108.0\n",
            "122.0\n",
            "112.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rewards = []\n",
        "for episode_idx in range(5):\n",
        "    obs, _ = env.reset()\n",
        "    reward_sum = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action, log_prob = select_action(policy_network, obs)\n",
        "        obs, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "        reward_sum += reward\n",
        "\n",
        "        done = terminated or truncated\n",
        "\n",
        "    rewards.append(reward_sum)\n",
        "\n",
        "print(rewards)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paD9tE1ZETyb",
        "outputId": "65afa1ba-0e63-41d9-a809-94d2940eb7da"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[114.0, 106.0, 118.0, 132.0, 110.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def select_greedy_action(policy_net, state):\n",
        "    features = get_features(state)\n",
        "    input = torch.tensor(features, dtype=torch.float32).to(device)\n",
        "    probs = policy_net(input)\n",
        "    m = torch.distributions.Categorical(probs)\n",
        "    return m.probs.argmax().item()\n",
        "\n",
        "rewards = []\n",
        "for episode_idx in range(25):\n",
        "    obs, _ = env.reset()\n",
        "    reward_sum = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action = select_greedy_action(policy_network, obs)\n",
        "        obs, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "        reward_sum += reward\n",
        "\n",
        "        done = terminated or truncated\n",
        "\n",
        "    rewards.append(reward_sum)\n",
        "\n",
        "print(rewards)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aB9SzO9LH34T",
        "outputId": "20e5f713-2666-44ac-9616-1b92b2b1f844"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[86.0, 104.0, 98.0, 93.0, 102.0, 110.0, 90.0, 120.0, 92.0, 114.0, 112.0, 102.0, 96.0, 106.0, 106.0, 100.0, 110.0, 93.0, 104.0, 106.0, 100.0, 86.0, 114.0, 116.0, 100.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tErsqwJcLTNS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}