{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72Q_qPRZqgmd",
        "outputId": "cf39725e-d17b-4499-c1d8-4cacaf22b202"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl.metadata (943 bytes)\n",
            "Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pygame pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHxHLJiyrXwd",
        "outputId": "08fd2846-9336-41ee-bbbb-8d7ba68b7a37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 1s (1,216 kB/s)\n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 126308 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Collecting box2d\n",
            "  Downloading Box2D-2.3.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (573 bytes)\n",
            "Collecting box2d-kengz\n",
            "  Downloading Box2D-kengz-2.3.3.tar.gz (425 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.4/425.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.14.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n",
            "Downloading Box2D-2.3.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-kengz\n",
            "  Building wheel for box2d-kengz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-kengz: filename=Box2D_kengz-2.3.3-cp311-cp311-linux_x86_64.whl size=2369340 sha256=ab6b743d27d795f78aa2de78ab4380ad9c55f517b5396b5ab622d5ffcadb0bd9\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/6a/a7/b49372347210a86f54c6603329c24c48dba26c98586ae80fbb\n",
            "Successfully built box2d-kengz\n",
            "Installing collected packages: box2d-kengz, box2d\n",
            "Successfully installed box2d-2.3.10 box2d-kengz-2.3.3\n"
          ]
        }
      ],
      "source": [
        "!apt install swig && pip install gymnasium box2d box2d-kengz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HZtoaO8loTWQ"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from IPython.display import display, HTML\n",
        "import base64\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIYGhHHMsv-Y",
        "outputId": "79243321-ab05-4ba4-88de-a80d66b6db86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuple(Discrete(32), Discrete(11), Discrete(2))\n",
            "Discrete(2)\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"Blackjack-v1\")\n",
        "print(env.observation_space)\n",
        "print(env.action_space)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.asarray(env.observation_space.sample())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yW2mfiQKRbC7",
        "outputId": "993e2111-cef6-417f-81d3-c79549f9044e"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(5)"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtoVjDdeYmwt",
        "outputId": "447129d3-8730-4cab-e976-c85134b78fd6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(0)"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "env.action_space.sample()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZJri0mEfef9J"
      },
      "outputs": [],
      "source": [
        "from itertools import combinations_with_replacement\n",
        "\n",
        "def polynomial_features(state, degree=1):\n",
        "    \"\"\"\n",
        "    Generate a polynomial basis feature vector from a state.\n",
        "\n",
        "    Parameters:\n",
        "        state: array-like\n",
        "        degree: int, the maximum total degree of polynomial terms\n",
        "\n",
        "    Returns:\n",
        "        feature_vector: np.ndarray, shape (num_features,)\n",
        "    \"\"\"\n",
        "    state = np.asarray(state)\n",
        "    size = state.shape[0]\n",
        "\n",
        "    # Create index tuples for all monomials up to the given degree\n",
        "    feature_vector = [1.0]  # bias term\n",
        "    for d in range(1, degree + 1):\n",
        "        for idxs in combinations_with_replacement(range(size), d):\n",
        "            term = 1.0\n",
        "            for i in idxs:\n",
        "                term *= state[i]\n",
        "            feature_vector.append(term)\n",
        "\n",
        "    return np.array(feature_vector).reshape(-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "glm35cNtzUfe"
      },
      "outputs": [],
      "source": [
        "from math import isinf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "alpha_sv_estimates = .001\n",
        "alpha_policy_updates = .0001\n",
        "gamma = 0.75 #discount factor on future rewards -- discounting pretty heavily\n",
        "# final episode reward should be allocated primarily to the last few state/action pairs and not earlier ones\n",
        "num_episodes = 20000\n",
        "dim = polynomial_features(env.observation_space.sample()).shape[0]\n",
        "wts = np.zeros(dim)\n",
        "num_actions = 2\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "epochs_per_batch = 5\n",
        "epsilon = .03\n",
        "\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 6)\n",
        "        self.fc2 = nn.Linear(6, 6)\n",
        "        self.fc3 = nn.Linear(6, action_dim)\n",
        "\n",
        "\n",
        "    def forward(self, state):\n",
        "\n",
        "        if torch.isnan(state).any():\n",
        "          print(\"NaNs in features!\")\n",
        "        if torch.isinf(state).any():\n",
        "          print(\"Infs in features!\")\n",
        "\n",
        "        x = F.relu(self.fc1(state))\n",
        "\n",
        "        if torch.isnan(x).any():\n",
        "          print(\"NaNs detected in output of layer 1!\")\n",
        "\n",
        "        if torch.isinf(x).any():\n",
        "          print(\"Infs detected in output of layer 1!\")\n",
        "\n",
        "        y = F.relu(self.fc2(x))\n",
        "\n",
        "        if torch.isnan(y).any():\n",
        "          print(\"NaNs detected in output of layer 2!\")\n",
        "\n",
        "        if torch.isinf(y).any():\n",
        "          print(\"Infs detected in output of layer 2!\")\n",
        "\n",
        "        logits = self.fc3(y)\n",
        "\n",
        "        if torch.isnan(logits).any():\n",
        "          print(\"NaNs detected in logits!\")\n",
        "\n",
        "        if torch.isinf(logits).any():\n",
        "          print(\"Infs detected in logits!\")\n",
        "\n",
        "\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        probs = probs / probs.sum(dim=-1, keepdim=True)  # ensures it sums to 1\n",
        "        return probs\n",
        "\n",
        "\n",
        "def gradient_descent_sv_weights(w, G, s):\n",
        "  features = polynomial_features(s)\n",
        "  w += alpha_sv_estimates * (G - np.dot(w, features)) * features\n",
        "  return w\n",
        "\n",
        "def select_action(policy_net, state):\n",
        "    features = polynomial_features(state)\n",
        "    if np.isnan(features).any():\n",
        "      print(\"NaNs in features!\")\n",
        "    if torch.isinf(torch.tensor(features)).any():\n",
        "      print(\"Infs in features!\")\n",
        "    input = torch.tensor(features, dtype=torch.float32).to(device)\n",
        "    probs = policy_net(input)\n",
        "    m = torch.distributions.Categorical(probs)\n",
        "    action = m.sample()\n",
        "    log_prob = m.log_prob(action)\n",
        "    return action.item(), log_prob\n",
        "\n",
        "def get_log_prob(s, a):\n",
        "  features = polynomial_features(s)\n",
        "  if np.isnan(features).any():\n",
        "    print(\"NaNs in features!\")\n",
        "  if torch.isinf(torch.tensor(features)).any():\n",
        "    print(\"Infs in features!\")\n",
        "  input = torch.tensor(features, dtype=torch.float32, device=device)\n",
        "  probs = policy_network(input)\n",
        "  m = torch.distributions.Categorical(probs)\n",
        "  log_prob = m.log_prob(torch.tensor(a, dtype=torch.long, device=device))\n",
        "  return log_prob\n",
        "\n",
        "\n",
        "def safe_normalize(tensor: torch.Tensor, eps=1e-8):\n",
        "    if torch.isnan(tensor).any():\n",
        "        return tensor  # skip normalization\n",
        "    if torch.isinf(tensor).any():\n",
        "        return tensor  # skip normalization\n",
        "\n",
        "    mean = tensor.mean()\n",
        "    std = tensor.std()\n",
        "\n",
        "    if torch.isnan(mean) or torch.isnan(std) or std < eps:\n",
        "        return tensor  # skip normalization\n",
        "\n",
        "    return (tensor - mean) / (std + eps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yElMuUJjCEb2",
        "outputId": "874ae481-ee2c-4b1e-9853-5fe589fb1e52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-88-4004337023.py:102: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1831.)\n",
            "  std = tensor.std()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 100 moving avg reward:  -0.9\n",
            "Episode: 200 moving avg reward:  -1.2\n",
            "Episode: 300 moving avg reward:  -1.1\n",
            "Episode: 400 moving avg reward:  -0.8\n",
            "Episode: 500 moving avg reward:  -0.45\n",
            "Episode: 600 moving avg reward:  -0.5\n",
            "Episode: 700 moving avg reward:  -0.7\n",
            "Episode: 800 moving avg reward:  -0.45\n",
            "Episode: 900 moving avg reward:  -0.9\n",
            "Episode: 1000 moving avg reward:  -0.75\n",
            "Episode: 1100 moving avg reward:  -0.75\n",
            "Episode: 1200 moving avg reward:  -0.95\n",
            "Episode: 1300 moving avg reward:  -0.6\n",
            "Episode: 1400 moving avg reward:  -0.85\n",
            "Episode: 1500 moving avg reward:  -0.7\n",
            "Episode: 1600 moving avg reward:  -1.3\n",
            "Episode: 1700 moving avg reward:  -0.8\n",
            "Episode: 1800 moving avg reward:  -0.55\n",
            "Episode: 1900 moving avg reward:  -0.8\n",
            "Episode: 2000 moving avg reward:  -0.85\n",
            "Episode: 2100 moving avg reward:  -1.65\n",
            "Episode: 2200 moving avg reward:  -1.15\n",
            "Episode: 2300 moving avg reward:  -1.05\n",
            "Episode: 2400 moving avg reward:  -0.8\n",
            "Episode: 2500 moving avg reward:  -1.25\n",
            "Episode: 2600 moving avg reward:  -1.35\n",
            "Episode: 2700 moving avg reward:  -0.9\n",
            "Episode: 2800 moving avg reward:  -0.6\n",
            "Episode: 2900 moving avg reward:  -0.65\n",
            "Episode: 3000 moving avg reward:  -0.45\n",
            "Episode: 3100 moving avg reward:  -0.4\n",
            "Episode: 3200 moving avg reward:  -1.4\n",
            "Episode: 3300 moving avg reward:  -1.05\n",
            "Episode: 3400 moving avg reward:  -0.65\n",
            "Episode: 3500 moving avg reward:  -0.85\n",
            "Episode: 3600 moving avg reward:  -0.9\n",
            "Episode: 3700 moving avg reward:  -0.45\n",
            "Episode: 3800 moving avg reward:  -1.25\n",
            "Episode: 3900 moving avg reward:  -0.8\n",
            "Episode: 4000 moving avg reward:  -0.35\n",
            "Episode: 4100 moving avg reward:  -0.6\n",
            "Episode: 4200 moving avg reward:  -0.85\n",
            "Episode: 4300 moving avg reward:  -0.9\n",
            "Episode: 4400 moving avg reward:  -1.25\n",
            "Episode: 4500 moving avg reward:  -0.8\n",
            "Episode: 4600 moving avg reward:  -1.05\n",
            "Episode: 4700 moving avg reward:  -1.0\n",
            "Episode: 4800 moving avg reward:  -0.45\n",
            "Episode: 4900 moving avg reward:  -0.85\n",
            "Episode: 5000 moving avg reward:  0.0\n",
            "Episode: 5100 moving avg reward:  -0.95\n",
            "Episode: 5200 moving avg reward:  -0.7\n",
            "Episode: 5300 moving avg reward:  -0.2\n",
            "Episode: 5400 moving avg reward:  -0.55\n",
            "Episode: 5500 moving avg reward:  -1.45\n",
            "Episode: 5600 moving avg reward:  -0.1\n",
            "Episode: 5700 moving avg reward:  -0.4\n",
            "Episode: 5800 moving avg reward:  0.05\n",
            "Episode: 5900 moving avg reward:  -0.7\n",
            "Episode: 6000 moving avg reward:  -0.65\n",
            "Episode: 6100 moving avg reward:  -0.25\n",
            "Episode: 6200 moving avg reward:  -0.7\n",
            "Episode: 6300 moving avg reward:  -0.35\n",
            "Episode: 6400 moving avg reward:  -1.1\n",
            "Episode: 6500 moving avg reward:  -0.15\n",
            "Episode: 6600 moving avg reward:  -0.5\n",
            "Episode: 6700 moving avg reward:  -1.2\n",
            "Episode: 6800 moving avg reward:  -0.9\n",
            "Episode: 6900 moving avg reward:  -0.25\n",
            "Episode: 7000 moving avg reward:  0.25\n",
            "Episode: 7100 moving avg reward:  -0.4\n",
            "Episode: 7200 moving avg reward:  -0.4\n",
            "Episode: 7300 moving avg reward:  -0.6\n",
            "Episode: 7400 moving avg reward:  -0.55\n",
            "Episode: 7500 moving avg reward:  -0.45\n",
            "Episode: 7600 moving avg reward:  -0.7\n",
            "Episode: 7700 moving avg reward:  -0.4\n",
            "Episode: 7800 moving avg reward:  0.0\n",
            "Episode: 7900 moving avg reward:  0.15\n",
            "Episode: 8000 moving avg reward:  -0.15\n",
            "Episode: 8100 moving avg reward:  -0.15\n",
            "Episode: 8200 moving avg reward:  0.25\n",
            "Episode: 8300 moving avg reward:  -0.5\n",
            "Episode: 8400 moving avg reward:  -0.6\n",
            "Episode: 8500 moving avg reward:  -0.25\n",
            "Episode: 8600 moving avg reward:  0.2\n",
            "Episode: 8700 moving avg reward:  -0.15\n",
            "Episode: 8800 moving avg reward:  -0.65\n",
            "Episode: 8900 moving avg reward:  -0.25\n",
            "Episode: 9000 moving avg reward:  -0.75\n",
            "Episode: 9100 moving avg reward:  -0.3\n",
            "Episode: 9200 moving avg reward:  -0.55\n",
            "Episode: 9300 moving avg reward:  -0.65\n",
            "Episode: 9400 moving avg reward:  0.0\n",
            "Episode: 9500 moving avg reward:  -0.4\n",
            "Episode: 9600 moving avg reward:  -0.55\n",
            "Episode: 9700 moving avg reward:  -0.1\n",
            "Episode: 9800 moving avg reward:  -0.5\n",
            "Episode: 9900 moving avg reward:  0.05\n",
            "Episode: 10000 moving avg reward:  -0.5\n",
            "Episode: 10100 moving avg reward:  -0.65\n",
            "Episode: 10200 moving avg reward:  -0.5\n",
            "Episode: 10300 moving avg reward:  -0.3\n",
            "Episode: 10400 moving avg reward:  -0.45\n",
            "Episode: 10500 moving avg reward:  -0.4\n",
            "Episode: 10600 moving avg reward:  -0.25\n",
            "Episode: 10700 moving avg reward:  -0.45\n",
            "Episode: 10800 moving avg reward:  0.1\n",
            "Episode: 10900 moving avg reward:  -0.65\n",
            "Episode: 11000 moving avg reward:  -0.1\n",
            "Episode: 11100 moving avg reward:  0.0\n",
            "Episode: 11200 moving avg reward:  0.15\n",
            "Episode: 11300 moving avg reward:  0.0\n",
            "Episode: 11400 moving avg reward:  -0.65\n",
            "Episode: 11500 moving avg reward:  -0.1\n",
            "Episode: 11600 moving avg reward:  -0.4\n",
            "Episode: 11700 moving avg reward:  -0.35\n",
            "Episode: 11800 moving avg reward:  0.2\n",
            "Episode: 11900 moving avg reward:  -0.2\n",
            "Episode: 12000 moving avg reward:  -0.1\n",
            "Episode: 12100 moving avg reward:  0.05\n",
            "Episode: 12200 moving avg reward:  -0.35\n",
            "Episode: 12300 moving avg reward:  -0.05\n",
            "Episode: 12400 moving avg reward:  -0.1\n",
            "Episode: 12500 moving avg reward:  0.05\n",
            "Episode: 12600 moving avg reward:  -0.3\n",
            "Episode: 12700 moving avg reward:  -0.05\n",
            "Episode: 12800 moving avg reward:  0.0\n",
            "Episode: 12900 moving avg reward:  -0.35\n",
            "Episode: 13000 moving avg reward:  0.15\n",
            "Episode: 13100 moving avg reward:  -0.1\n",
            "Episode: 13200 moving avg reward:  -0.35\n",
            "Episode: 13300 moving avg reward:  -0.9\n",
            "Episode: 13400 moving avg reward:  -0.45\n",
            "Episode: 13500 moving avg reward:  -0.15\n",
            "Episode: 13600 moving avg reward:  -0.45\n",
            "Episode: 13700 moving avg reward:  -0.2\n",
            "Episode: 13800 moving avg reward:  -0.15\n",
            "Episode: 13900 moving avg reward:  0.4\n",
            "Episode: 14000 moving avg reward:  0.0\n",
            "Episode: 14100 moving avg reward:  -0.05\n",
            "Episode: 14200 moving avg reward:  -0.25\n",
            "Episode: 14300 moving avg reward:  -0.25\n",
            "Episode: 14400 moving avg reward:  0.3\n",
            "Episode: 14500 moving avg reward:  -0.05\n",
            "Episode: 14600 moving avg reward:  -0.25\n",
            "Episode: 14700 moving avg reward:  -0.95\n",
            "Episode: 14800 moving avg reward:  -0.1\n",
            "Episode: 14900 moving avg reward:  0.1\n",
            "Episode: 15000 moving avg reward:  0.2\n",
            "Episode: 15100 moving avg reward:  -0.05\n",
            "Episode: 15200 moving avg reward:  -0.1\n",
            "Episode: 15300 moving avg reward:  -0.3\n",
            "Episode: 15400 moving avg reward:  0.1\n",
            "Episode: 15500 moving avg reward:  -0.35\n",
            "Episode: 15600 moving avg reward:  0.8\n",
            "Episode: 15700 moving avg reward:  -0.15\n",
            "Episode: 15800 moving avg reward:  -0.35\n",
            "Episode: 15900 moving avg reward:  -0.4\n",
            "Episode: 16000 moving avg reward:  -0.1\n",
            "Episode: 16100 moving avg reward:  -0.35\n",
            "Episode: 16200 moving avg reward:  0.1\n",
            "Episode: 16300 moving avg reward:  -0.1\n",
            "Episode: 16400 moving avg reward:  -0.35\n",
            "Episode: 16500 moving avg reward:  -0.05\n",
            "Episode: 16600 moving avg reward:  0.0\n",
            "Episode: 16700 moving avg reward:  -0.5\n",
            "Episode: 16800 moving avg reward:  0.3\n",
            "Episode: 16900 moving avg reward:  -0.05\n",
            "Episode: 17000 moving avg reward:  0.05\n",
            "Episode: 17100 moving avg reward:  0.15\n",
            "Episode: 17200 moving avg reward:  -0.15\n",
            "Episode: 17300 moving avg reward:  0.15\n",
            "Episode: 17400 moving avg reward:  0.05\n",
            "Episode: 17500 moving avg reward:  -0.3\n",
            "Episode: 17600 moving avg reward:  0.25\n",
            "Episode: 17700 moving avg reward:  -0.35\n",
            "Episode: 17800 moving avg reward:  -0.05\n",
            "Episode: 17900 moving avg reward:  -0.25\n",
            "Episode: 18000 moving avg reward:  -0.25\n",
            "Episode: 18100 moving avg reward:  -0.35\n",
            "Episode: 18200 moving avg reward:  -0.2\n",
            "Episode: 18300 moving avg reward:  -0.25\n",
            "Episode: 18400 moving avg reward:  -0.05\n",
            "Episode: 18500 moving avg reward:  0.35\n",
            "Episode: 18600 moving avg reward:  -0.35\n",
            "Episode: 18700 moving avg reward:  -0.45\n",
            "Episode: 18800 moving avg reward:  -0.15\n",
            "Episode: 18900 moving avg reward:  0.1\n",
            "Episode: 19000 moving avg reward:  -0.55\n",
            "Episode: 19100 moving avg reward:  -0.55\n",
            "Episode: 19200 moving avg reward:  -0.25\n",
            "Episode: 19300 moving avg reward:  -0.15\n",
            "Episode: 19400 moving avg reward:  0.3\n",
            "Episode: 19500 moving avg reward:  0.0\n",
            "Episode: 19600 moving avg reward:  0.25\n",
            "Episode: 19700 moving avg reward:  -0.25\n",
            "Episode: 19800 moving avg reward:  -0.05\n",
            "Episode: 19900 moving avg reward:  0.05\n",
            "Episode: 20000 moving avg reward:  -0.15\n"
          ]
        }
      ],
      "source": [
        "policy_network = PolicyNetwork(dim, num_actions).to(device)\n",
        "optimizer = torch.optim.Adam(policy_network.parameters(), lr=alpha_policy_updates)\n",
        "episode_rewards = []\n",
        "for episode_idx in range(num_episodes):\n",
        "    rewards = [] #stores rewards of each time step in episode\n",
        "    actions = [] #stores actions taken in episode under behavior policy\n",
        "    states = [] #stores states visited in episode by behavior policy\n",
        "    probs = [] #stores pi(a|s) for each state,action pair in episode under behavior policy; detached from computation graph\n",
        "\n",
        "    obs, _ = env.reset()\n",
        "    undiscounted_reward_sum = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        states.append(obs)\n",
        "        action, log_prob = select_action(policy_network, obs) #forward pass thru policy network using cur state (obs) and returns action (discrete idx) and log_prob of that action\n",
        "        prob = torch.exp(log_prob).detach() #probability of the discrete action BUT NOT the sampled action; need to divide\n",
        "        probs.append(prob)\n",
        "        obs, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "\n",
        "        actions.append(action) #store the discrete action index\n",
        "\n",
        "        done = terminated or truncated\n",
        "        if not done:\n",
        "          rewards.append(reward) #store reward from this step\n",
        "        else: #end of blackjack round\n",
        "          if action == 1: #last action was hit means that you busted\n",
        "            rewards.append(-1)\n",
        "          else: #last action was a stick\n",
        "            if obs[0] <= 11: #stuck with <= 11 is an awful move\n",
        "              rewards.append(-5)\n",
        "            else:\n",
        "              rewards.append(reward)\n",
        "        undiscounted_reward_sum += rewards[-1]\n",
        "    episode_rewards.append(undiscounted_reward_sum)\n",
        "\n",
        "    if (episode_idx+1) % 100 == 0:\n",
        "        print('Episode: {} moving avg reward: '.format(episode_idx+1), np.mean(episode_rewards[-20:]))\n",
        "\n",
        "    # Compute discounted returns\n",
        "    returns = []\n",
        "    G = 0\n",
        "    for r in reversed(rewards):\n",
        "        G = r + gamma * G\n",
        "        returns.insert(0, G)\n",
        "\n",
        "    # Compute advantages and update baseline weights\n",
        "    advantages = []\n",
        "    for i in range(len(states)):\n",
        "        cur_state = states[i]\n",
        "        cur_return = returns[i]\n",
        "        feat_np = polynomial_features(cur_state)\n",
        "        baseline = np.dot(wts, feat_np)\n",
        "        adv = cur_return - baseline\n",
        "        advantages.append(adv)\n",
        "\n",
        "\n",
        "        #wts = gradient_descent_sv_weights(wts, cur_return, cur_state)\n",
        "\n",
        "    advantages = torch.tensor(advantages, dtype=torch.float32, device=device)\n",
        "    safe_normalize(advantages)\n",
        "\n",
        "    for i in range(epochs_per_batch):\n",
        "      clipped_vals = []\n",
        "      unclipped_vals = []\n",
        "\n",
        "      state_features_np = np.array([polynomial_features(s) for s in states])  # shape: [batch_size, feature_dim]\n",
        "      states_tensor = torch.tensor(state_features_np, dtype=torch.float32, device=device)\n",
        "      actions_tensor = torch.tensor(actions, dtype=torch.long, device=device)\n",
        "      old_probs_tensor = torch.tensor(probs, dtype=torch.float32, device=device)\n",
        "\n",
        "      logits = policy_network(states_tensor) #forward pass using visited states thru the policy network which outputs a distribution over the discrete action space for each state (ie: batch size x 10 ** 4)\n",
        "      dists = torch.distributions.Categorical(logits)\n",
        "      new_log_probs = dists.log_prob(actions_tensor)  # shape: [batch]; log_prob of the discrete action index!\n",
        "      new_probs = torch.exp(new_log_probs)\n",
        "\n",
        "      # Compute ratios\n",
        "      ratios = new_probs / (old_probs_tensor + 1e-8)  # shape: [batch]\n",
        "\n",
        "\n",
        "      # PPO clipped objective (vectorized)\n",
        "      clipped_ratios = torch.clamp(ratios, 1 - epsilon, 1 + epsilon)\n",
        "      clipped_obj = clipped_ratios * advantages\n",
        "      unclipped_obj = ratios * advantages\n",
        "\n",
        "      loss_per_step = -torch.min(unclipped_obj, clipped_obj)  # shape: [batch]\n",
        "      loss = loss_per_step.mean()\n",
        "\n",
        "      if torch.isnan(loss):\n",
        "        print(\"NaN loss detected\")\n",
        "        print(\"ratios:\", ratios)\n",
        "        print(\"advantages:\", advantages)\n",
        "        print(\"unclipped:\", unclipped_obj)\n",
        "        print(\"clipped:\", clipped_obj)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "\n",
        "      for name, param in policy_network.named_parameters():\n",
        "        if param.grad is not None and torch.isnan(param.grad).any():\n",
        "          print(f\"NaN in gradient of {name}\")\n",
        "        if param.grad is not None and torch.norm(param.grad) > 1e6:\n",
        "          print(f\"Exploding gradient in {name}:\", torch.norm(param.grad).item())\n",
        "      torch.nn.utils.clip_grad_norm_(policy_network.parameters(), max_norm=1.0)\n",
        "      optimizer.step()\n",
        "\n",
        "    for i in range(len(states)):\n",
        "        cur_state = states[i]\n",
        "        cur_return = returns[i]\n",
        "        wts = gradient_descent_sv_weights(wts, cur_return, cur_state)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paD9tE1ZETyb",
        "outputId": "2a4f53c9-0175-47bd-920a-3160ef636ff1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.064\n"
          ]
        }
      ],
      "source": [
        "rewards = []\n",
        "for episode_idx in range(1000):\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action, log_prob = select_action(policy_network, obs)\n",
        "        obs, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "        done = terminated or truncated\n",
        "        if done:\n",
        "          rewards.append(reward)\n",
        "\n",
        "print(sum(rewards)/len(rewards))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lose a tad more more than we win -- which is not unexpected in Blackjack"
      ],
      "metadata": {
        "id": "apJLLNh_1byn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prob(s):\n",
        "  features = polynomial_features(s)\n",
        "  if np.isnan(features).any():\n",
        "    print(\"NaNs in features!\")\n",
        "  if torch.isinf(torch.tensor(features)).any():\n",
        "    print(\"Infs in features!\")\n",
        "  input = torch.tensor(features, dtype=torch.float32, device=device)\n",
        "  probs = policy_network(input)\n",
        "  return probs\n",
        "\n",
        "ct = 0\n",
        "while True:\n",
        "  state = env.observation_space.sample()\n",
        "  if state[0] <= 20:\n",
        "    ct +=1\n",
        "    print(state, get_prob(state), sep = ': ')\n",
        "    if ct == 50:\n",
        "      break\n",
        "  else:\n",
        "    continue\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFdZfh6k1qe3",
        "outputId": "63b47753-fea5-4477-c6f9-17caf6dbcf0d"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(np.int64(20), np.int64(2), np.int64(0)): tensor([0.9883, 0.0117], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(0), np.int64(2), np.int64(1)): tensor([0.0074, 0.9926], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(19), np.int64(7), np.int64(0)): tensor([0.9754, 0.0246], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(8), np.int64(9), np.int64(0)): tensor([0.0021, 0.9979], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(0), np.int64(2), np.int64(1)): tensor([0.0074, 0.9926], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(1), np.int64(6), np.int64(0)): tensor([0.0057, 0.9943], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(9), np.int64(2), np.int64(0)): tensor([0.0108, 0.9892], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(16), np.int64(1), np.int64(0)): tensor([0.7634, 0.2366], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(10), np.int64(7), np.int64(1)): tensor([0.0270, 0.9730], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(7), np.int64(0), np.int64(1)): tensor([0.0048, 0.9952], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(6), np.int64(4), np.int64(1)): tensor([0.0029, 0.9971], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(15), np.int64(3), np.int64(1)): tensor([0.7685, 0.2315], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(3), np.int64(9), np.int64(0)): tensor([0.0033, 0.9967], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(0), np.int64(2), np.int64(0)): tensor([0.0063, 0.9937], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(14), np.int64(0), np.int64(1)): tensor([0.5873, 0.4127], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(0), np.int64(3), np.int64(1)): tensor([0.0083, 0.9917], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(9), np.int64(1), np.int64(1)): tensor([0.0240, 0.9760], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(10), np.int64(7), np.int64(1)): tensor([0.0270, 0.9730], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(4), np.int64(9), np.int64(0)): tensor([0.0030, 0.9970], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(8), np.int64(5), np.int64(0)): tensor([0.0050, 0.9950], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(13), np.int64(8), np.int64(1)): tensor([0.2587, 0.7413], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(11), np.int64(5), np.int64(0)): tensor([0.0546, 0.9454], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(6), np.int64(5), np.int64(0)): tensor([0.0040, 0.9960], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(13), np.int64(7), np.int64(1)): tensor([0.4054, 0.5946], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(11), np.int64(9), np.int64(1)): tensor([0.0177, 0.9823], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(0), np.int64(2), np.int64(0)): tensor([0.0063, 0.9937], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(1), np.int64(7), np.int64(0)): tensor([0.0051, 0.9949], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(5), np.int64(4), np.int64(0)): tensor([0.0031, 0.9969], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(11), np.int64(6), np.int64(0)): tensor([0.0551, 0.9449], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(12), np.int64(4), np.int64(1)): tensor([0.2262, 0.7738], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(15), np.int64(1), np.int64(0)): tensor([0.5885, 0.4115], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(9), np.int64(7), np.int64(0)): tensor([0.0056, 0.9944], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(5), np.int64(2), np.int64(1)): tensor([0.0011, 0.9989], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(15), np.int64(0), np.int64(1)): tensor([0.7625, 0.2375], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(10), np.int64(3), np.int64(0)): tensor([0.0244, 0.9756], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(7), np.int64(3), np.int64(0)): tensor([0.0022, 0.9978], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(10), np.int64(0), np.int64(1)): tensor([0.0521, 0.9479], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(0), np.int64(4), np.int64(0)): tensor([0.0079, 0.9921], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(4), np.int64(1), np.int64(0)): tensor([7.6476e-04, 9.9924e-01], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(8), np.int64(10), np.int64(0)): tensor([0.0019, 0.9981], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(19), np.int64(6), np.int64(0)): tensor([0.9751, 0.0249], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(1), np.int64(9), np.int64(0)): tensor([0.0040, 0.9960], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(8), np.int64(10), np.int64(1)): tensor([0.0017, 0.9983], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(4), np.int64(8), np.int64(1)): tensor([0.0032, 0.9968], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(3), np.int64(9), np.int64(1)): tensor([0.0031, 0.9969], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(17), np.int64(1), np.int64(0)): tensor([0.8792, 0.1208], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(11), np.int64(0), np.int64(1)): tensor([0.1102, 0.8898], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(6), np.int64(8), np.int64(1)): tensor([0.0026, 0.9974], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(10), np.int64(0), np.int64(0)): tensor([0.0242, 0.9758], device='cuda:0', grad_fn=<DivBackward0>)\n",
            "(np.int64(14), np.int64(4), np.int64(0)): tensor([0.3959, 0.6041], device='cuda:0', grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "seems like a decent policy is learned"
      ],
      "metadata": {
        "id": "rJ4lM-Pe44ME"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNHsBRx1BKY+nb43/5n3aat"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}